day03

3일차 수업예제 링크
1) https://colab.research.google.com/drive/1TlT9v0YKhh9lim6DhlQYTKSQYU5Xw-je?usp=sharing
2) https://colab.research.google.com/drive/1tLbYdLxKs248BUKaThOPq5_Y1ZWSkeug?usp=sharing
3) https://colab.research.google.com/drive/1AKXyNa2a7S7tS7HoPrsmPXfwDW-dA85B?usp=sharing
4) https://colab.research.google.com/drive/1CZO558jLGEqw-r6i20AlQ-TRx2WHsqU3?usp=sharing
5) https://colab.research.google.com/drive/16cTPuHMDidY-v30Ozl0T6kjwkOX9JXpv?usp=sharing

----------------------------------------------------------

SVM(Support Vector Machine)
	주어진 데이터를 분류하거나 회귀하는데 사용하는 지도학습 알고리즘

PCA(Principal Component Analysis)
	주로 데이터의 차원을 줄이기 위해서 사용됨

1) PCA로 차원을 축소시키는 이유
- 시각화 : 고차원 데이터는 시각화하기 어렵기 때문에 PCA를 사용하여 데이터를
	2차원 또는 3차원으로 축소해서 시각화
- 계산 효율성 : 고차원 데이터셋은 계산 비용이 높고, 메모리 요구량도 크기 때문에
	학습 및 예측 시 시간이 오래 걸림 -> 데이터의 크기를 줄여 계산 비용을 절감시킬수있음
- 고차원 데이터의 문제점 해결 : 고차원 데이터에는 많은 특성이 포함되어 있을 수 있으므로 
	과적합을 유발할 수 있고, 특성간의 상관관계가 높아
	선형모델에서 문제 발생가능성이 있음 -> PCA를 사용하여 주요 특성만 남기고 노이즈를
	제거함으로써 문제를 완화할 수 있음

2) PCA를 진행 후 분류 모델을 적용하는 이유
- 복잡도 감소 : PCA를 사용하여 데이터의 차원을 줄이면 모델의 복잡도를 줄일 수 있음
	-> 모델이 과적합되는 경향을 줄이고, 일반화 성능을 향상시킬 수 있음
- 선형 분리 가능성 : 고차원 공간에서는 데이터가 선형적으로 분리될 가능성이 높음
	-> PCA를 사용하여 데이터의 차원을 줄이면 SVM과 같은 선형분류기가 잘 작동할 수 있음
- 차원의 저주 완화 : 고차원 데이터의 경우 차원의 저주(curse of dimensionality)로 인해
	학습 데이터가 희소해질 수 있음 -> PCA 사용 후 분류 모델을 적용하면 이러한 문제를
	완화할 수 있음

2. 선형 모델 VS 비선형 모델
1) 선형모델(Linear Model)
	입력특성들 간의 선형관계를 사용하여 출력을 예측하는 모델
	선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression) 가장 잘 알려진 선형모델
	선형 모델은 특성 공간에서 선형 경계를 갖는다
	데이터 시각화 시 특성 간의 관계가 직선으로 나타나는지 확인 할 수 잇다
	상관관계분석 선형적인 관계 있는지 확인 할 수 있다(상관관계가 높은 톡성들은 선형 모델, 
	상관관계가 낮거나 없는 특성들은 비선형 모델을 고려해야할 수도 있다)
	모델 성능 평가  : 모델 학습 후 테스트하여 성능을 평가하는 과정에서 모델을 성능을 파악할 수 있다
	(모델이 선형모델과 비선형모델 중 어떤 모델이 더 나은 성능을 보이는지 확인할 수 있다)
	특성의 개수가 많지 않고 특성간의 선형관계 존재할 때 잘 작동한다
	
2) 비선형모델(Nonlinear Model)
	입력 특성들간의 비선형관계를 사용하여 출력을 예측하는 모델
	다항회귀, 의사결정나무, 신경망 등이 비선형모델에 속함
	특성공간에서 직선이 아닌 곡선의 경계를 가질 수 있다
	선형모델로는 적절한 경계를 찾기 어려운 경우 비선형모델을 사용한다

선형모델 : 모수(파라미터)들이 선형 함수로 주어지는 모형을 의미
	모델의 예측이 모수(파라미터)들의 선형 함수로 주어지는 것을 의미, 입력변수들의 가중치를 곱한 후
	이를 모두 합산하여 예측을 수행하는 것을 의미한다
	Y = w0 + x₁x₁ + .. + w_nx_n
	Y는 종속변수(예측하려는 값), x₁은 독립변수(입력변수), w₁ 독립변수의 가중치(계수)
	예측값을 계산하는 과정은 입력변수들의 선형 조합으로 이루어진다

비선형모델 : 다항식, 지수, 로그, 삼각함수 등의 함수를 사용
	입력변수와 종속변수 간의 관계 선형적이지 않을 때 사용된다
	입력변수들의 가중치를 곱한 후 비선형 함수에 적용하여 예측을 수행한다
	
	Y = f(w0 + x₁x₁ + .. + w_nx_n)
	f() 비선형 함수를 의미, y는 종속변수(예측하려는 값), x1은 독립변수(입력변수), 
	w₁ 각 독립변수의 가중치(계수)	
	
3. 앙상블(Ensemble) 기법
	여러개의 기본 모델을 활용하여 하나의 새로운 모델을 만들어 내는 것
	모델의 예측성능을 향상시키기 위해 사용

1) 보팅(Voting)
	서로 다른 알고리즘을 가진 분류기를 결합하는 방식
	하드보팅(Hard Voting) : 다수결 원칙 기반, 예측 결과값들 다수 분류기가 결정한 예측값을
		최종 보팅 결과값으로 선택함
		(각 분류기의 예측 결과중 가장 많은 것을 선택-다수결 원칙)
	소프트보팅(Soft Voting) : 레이블값 결정 확률을 모두 더하고 평균 낸 확률이 가장 높은 레이블값을
		최종 보팅 결과값으로 선택함
		(각 분류기의 예측 결과에 대한 확률을 평균하여 가장 높은 확률을 가진 클래스를 선택)

2) 배깅(Bagging)
	Bootstrap Aggregating 줄임말
	데이터를 재구성하여 다양한 모델을 만들어내는 앙상블 기법
	의사결정트리 처럼 과적합이 발생하기 쉬운 모델에서 사용
	랜덤포레스트는 배깅을 기반으로 한 모델로 여러개의 의사결정트리를 생성하고 그 결과를 조합하여 다양한
	예측을 수행한다

3) 부스팅(Boosting)
	이전모델들이 잘못 예측한 데이터에 높은 가중치를 주어 학습하는 방식
	Adaptive Boosting(AdaBoost) : 오분류된 데이터에 가중치를 부여하면서 부스팅을 수행
		이전 분류기들의 오류를 보완하는 방식으로 새로운 분류기를 학습
	Gradinet Boosting : 이전 라운드에서 합성분류기 별 오류를 새로운 약한 분류기를 학습
		경사하강법을 통해 오류를 최소화 하는 방법으로 모델 향상


































