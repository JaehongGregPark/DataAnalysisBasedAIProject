day05
내적 -> 통계 -> 딥러닝 개념

1. AI에서 필요한 수학
1) 선형대수학
	벡터와 행렬을 다루는 수학의 한 분야
	AI에서 데이터는 대부분 벡터나 행렬의 형태로 표현된다
	벡터 : 크기와 방향을 가지는 양, 여러개의 숫자가 보여있다
	행렬 : 숫자들이 행과 열로 구성된 배열

2) 미적분학
	변화를 다루는 수학의 한 분야, 함수의 도함수(미분)과 적분을 다룬다
	머신러닝이나 딥러닝에서 손실함수를 최적화하기 위해 미분을 사용한다
	역전파알고리즘에서도 미분이 사용되고 중요한 역할을 한다

3) 확률과 통계
	데이터를 분석하고 모델을 평가하는데 사용된다
	확률은 불확실성을 다루는 수학, 데이터 분포를 모델링하고 예측하기 위해 사용된다
	통계는 데이터를 수집, 분석하여 패턴을 찾고 결론을 도출하는데 사용된다
	
4) 최적화
	주어진 조건 하에서 목적 함수를 최대화하거나 최소화 하는 문제를 다루는 수학의 한 분야
	머신러닝에서는 모델의 매개변수를 조정하여 손실함수를 최소화하는 것이 목표.
	경사하강법과 같은 최적화 알고리즘에서 활용된다

2. 내적(Inner Product)
	벡터 + 벡터 = 벡터
	스칼라 + 스칼라 = 스칼라
	1 + 5 = 6
	
	벡터  벡터 = 스칼라
	<u, v> = u·v = u₁v₁ + u₂v₂ + ... u_nv_n

- 내적을 이용하면 벡터의 길이(norm)을 구하거나 벡터 사이의 관계를 파악할 수 있다
- 내적을 이용하면 두 벡터 사이의 각도도 추정할 수 있다
	(1) 내적 > 0이면, 두 벡터 사이의 각도 < 90
	(2) 내적 < 0이면, 두 벡터 사이의 각도 > 90
	(3) 내적 = 0이면, 두 벡터 사이의 각도 = 90

1) 정사영(projection)
	한 벡터가 다른 벡터에 수직으로 투영하는 것을 의미한다
	내적은 정사영한 벡터의 길이와 연관되어 있다

- 내적은 좌표계의 선택과 무관하며 특정 좌표계에서만 내적을 할 수 있는 것이 아니라
 좌표계의 종류와 상관없이 내적값을 항상 구할 수 있다는 의미이다

- 그림 참고
	벡터 u를 벡터 v에 정사영시킨 벡터의 길이는 ∥u∥ |cos |임을 알 수 있다
	길이는 음수가 될 수 없으므로 코사인에 절대값을 취한다
	벡터 u를 벡터 v에 정사영시킨 벡터를 pro j u로 표기했을 때 해당 벡터의 길이는
	
	정사영 개념을 이용하여 내적 개념을 보면
	벡터 u와 벡터 v의 내적이란 벡터 u를 벡터 v에 정사영시킨 벡터의 길이다
	즉, ∥u∥ |cos |와 기존 벡터 v의 길이인 ∥v∥의 곱과 같다
	u·v = ∥u∥∥v∥|cos | = (∥v∥) x (∥u∥|cos |)

2) 선형 변환(linear transformation)
	두 벡터 공간 사이의 함수
	벡터 공간 내의 벡터를 다른 벡터로 변환하는 함수
	
	덧셈에 대한 보존성 : 두 벡터의 합의 변환은 각 벡터를 변환한 후 합한 것과 같다
	T(u + v) = T(u) + T(v) 가 되어야한다
	스칼라 곱에 대한 보존성 : 스칼라에 대한 곱의 변환은 해당 스칼라를 벡터에 곱한 후
		변환한 것과 같아야한다
	T(cu) = cT(u)

	T(x) = Ax
	A : 변환을 정의하는 행렬
	x : 입력 벡터
	행렬A : 입력공간과 출력공간사이의 선형관계를 나타낸다
	
	머신러닝에서는 데이터 전처리, 특성변환 시 선형변환을 사용한다
	


3. 통계학과 머신러닝의 관계
	머신러닝의 이론적인 배경은 통계학에 기반한 경우가 많기 때문에
	머신러닝 알고리즘을 이해하고 응용하기 위해 기본적인 통계 개념이 필요하다

4. 통계 분석
1) 모집단과 표본
	모집단 : 우리가 알고자하는 전체, 유용한 정보의 대상
		모집단을 구성하는 개체를 추출 단위 혹은 원소라고한다
	총조사(census) 방법 : 모집단의 개체 모두를 조사하는 방법으로 많은 비용과 시간이 소요됨
	표본조사 방법 : 표본(sample) -> 조사하는 모집단의 일부분
			모수(parameter) -> 모집단에서 알고자하는 값
			통계량(statistic) -> 모수를 추론하기 위해 구하는 표본의 값들
	유한 모집단 : 유한개의 개체로 이루어진 모집단
	무한 모집단 : 무한개의 개체로 이루어진 모집단, 개념적으로 상정된 모집단

2) 표본 추출의 방법
	단순랜덤추출 방법	N개의 원소로 구성된 모집단에서 n개의 표본을 추출할 때 각 원소에 1부터 N까지의
			번호를 부여하고 여기서 n개의 원소를 임의로 선택하여 그 번호에 해당하는
			원소를 표본으로 추출한다
	계통추출방법	모집단의 모든 원소들에게 1부터 N의 일련번호를 부여하고 이를 순서대로 나열한 뒤
			K개(K = N/n)씩 n개의 구간으로 나뉜다
			첫구간(1,2,...K)에서 하나를 임의로 선택한 후 K개씩 띄어서 표본을 추출한다
	집략추출법		모집단이 몇개의 집락(cluster)이 결합된 형태로 구성되어있고 각 집단에서 원소들에게
			일련번호를 부여할 수 있는 경우에 이용된다
			일부 클러스터를 랜덤으로 선택하고 선택된 각 클러스터에서 표본을 임의로 선택한다
	충화추출법		이질적인 원소들로 구성된 모집단에서 각 계층을 대표할 수 있도록 표본을 추출하는방법
			이질적인 모집단의 원소들을 서로 유사한 것끼리 몇개의 층으로 나눈 후 각 층에서
			표본을 랜덤하게 추출한다

3) 통계분석(statistical analysis)
	특정한 집단이나 불확실성한 현상을 대상으로 자료를 수집해 대상 집단에 대한 정보를 구하고,
	적절한 통계분석 방법을 이용해 의사결정을 하는 과정을 의미한다
	통계학에서는 통계적 추론이라고하고 대상 집단에 대한 정보란 자료를 요약하고 정리한 결과로 숫자 또는
	그림으로 정리된 각종 통계를 의미한다
	통계적 추론이란 수집된 자료를 이용해 대상 집단(모집단)에 대해 의사결정을 하는 것이다

- 대상 집단의 특성값(모수)이 무엇일까를 추측하는 추정(estimation)
- 대상 집단에 대해 특정한 가설을 설정한 후에 그 가설의 채택여부를 결정하는 가설검정(hypothesis test)
- 미래의 불확실성을 해결해 효율적인 의사 결정을 하기 위해 수행하는 예측(forecasting)
- 기술통계(descriptive statistic) : 통계적 추론 이외에도 수집된 자료를 정리, 요약하기 위해 사용되는
	기초적인 통계, 평균, 표준편차, 중위수, 최빈값 %와 같이 숫자로 표현하는 방식(메소드 describe())
	그래프와 같이 그림으로 표현하는 방식이 있다

5. 확률 및 확률 분포
1) 확률 변수
확률(probability) 	어떤 사건이 일어날 가능성을 수치화시킨 것을 의미한다
	(1) 모든 확률이 0과 1사이에 있어야한다
		0 <= P(A) <= 1
	(2) 발생 가능한 모든 사건의 확률을 모두 더하면 1이다
		P(S) = 1
		표본공간(sample space) : 통계적 실험을 실시할 때 나타날수있는 모든 결과의 집합
	(3) 동시에 발생할 수 없는 사건들에 대해 각 사건의 합의 확률은 개별확률이 일어날 확률의 합과 같다
		상호 배반 사건 : 교집합이 공집합인 사건을 의미
	
확률변수(random variable)	결과값이 확률적으로 정해지는 변수
함수(function) 	한 집합의 임의의 한 원소를 다른 집합의 한 원소에 대응시키는 관계
		확률변수가 취할 수 있는 값이 다수 존재할 때 확률변수는 사건을 확률에 대응시키는 함수

확률분포(probability distribution)
	확률 변수가 특정값을 가질 확률의 함수를 의미
	확률 변수가 특정값을 가질 확률이 얼마나 되느냐를 나타내는 것

- 이산형 확률 변수 : 사건의 확률이 그 사건들이 속한 점들의 확률의 합으로 표현할 수 있는 확률 변수
- 연속형 확률 변수 : 사건의 확률이 그 사건 위에서 어떤 0보다 큰 값을 갖는 함수의 면적으로 표현될 수 있는 확률 변수
- 결합확률 분포 : 두 확률 변수 X, Y의 결합확률 분포는 이산형인 경우 P(X=x_i, Y=y_j) = P_ij
		연속형인 경우 f(x, y)라고 정의하며 각각 결합확률질량함수와 결합확률밀도함수라고 한다

+) 연속 확률 변수
	확률 변수가 가질 수 있는 값의 개수를 셀 수 없다는 의미
	연속 확률 분포는 연속 확률 변수의 확률 분포를 의미한다
	
+) 확률 밀도 함수
	연속 확률 변수의 분포를 나타내는 함수
	이산 확률 분포에서의 확률 질량함수에 대응된다(특정값을 가질 확률은 0이 되므로 특정값을 가질 확률은
	확률 밀도 함수의 특정 구간에 포함된다)

- 누적 분포 함수
	주어진 확률 변수가 특정값보다 작거나 같은 확률을 나타내는 함수
	x축의 값이 증가할수록 확률을 적용하는 구간이 계속 누적되는 개념이므로 x값이 증가할수록 그래프가
	우상향하는 형태를 보인다

- 결합 확률 밀도 함수
	확률 변수 여러개를 함께 고려하는 확률 분포
	확률 변수 X가 x일 사건과 확률변수 Y가 y일 사건이 동시에 발생하는 상황
	확률 변수가 독립적인 경우에는 해당 사건이 동시에 일어날 확률은 각각 확률 변수의 확률밀도함수를 곱하는
	것과 같다

- 독립 항등 분포(iid, Independent and Identically distributed)
	두 개 이상의 확률 변수를 고려할 때 각 확률 변수가 통계적으로 독립이고, 
	동일한 확률 분포를 따르는것을 의미


6. 평균과 분산
+) 
모집단(population) 	관심이 있는 대상 전체
표본(sample)		모집단에서 일부를 추출한 것을 의미
모수(population parameter) 모집단의 특성을 나타내는 대표값
표본통계량(sample statistic) 표본의 대표값

1) 평균

산술평균 
	단순히 모든 데이터값을 덧셈한 후 데이터의 개수로 나누는 것을 의미한다
	E(X) = μ

모평균(population mean)
	모집단의 평균
	
+) 평균과 같이 그래프의 위치를 결정하는 파라미터를 로케이션 파라미터(Location prarmeter)

2. 분산(variance)
	데이터가 얼마나 퍼져있는지를 수치화한 것
	평균에 대한 편차 제곱의 평균으로 계산한다

모분산(populaticon variance)
	모집단의 모든 값에 대한 평균적인 편차의 제곱을 나타낸다 
	
모집단이 아닌 표본의 경우, 표본분산(sample variance)으로 계산한다
	모분산을 추정하는데 사용되며, 모집단의 모평균 대신 표본의 평균을 사용하여 계산된다

2) 공분산(covariance)
	두 확률 변수의 상관관계를 나타내는 값
	두 개의 확률 변수 중 하나의 값이 증가할 때 다른 값도 증가한다면 공분산은 양수
	하나의 값이 증가할 때 다른 값은 감소한다면 공분산은 음수

	(1) 만약 Cov(X, Y) > 0 라면 X와 Y는 양의 상관관계
	(2) 만약 Cov(X, Y) < 0 라면 X와 Y는 음의 상관관계
	(3) 만약 Cov(X, Y) = 0 이라면 X와 Y는 상관관계없음

3) 공분산 행렬
	확률변수 간 분산, 공분산을 행렬로 표현한 것을 의미
	머신러닝에서 차원축소할 때 자주 사용된다
	주어진 데이터 세트의 각 변수들 간의 상관관계와 분포를 나타내는데 사용된다
	
	특성 벡터들의 결합 분산과 각 특성들간의 상호관계를 포함한다
	특성 벡터 : 여러 차원의 데이터(각 차원은 해당 데이터의 하나의 특성을 나타냄)
	
4) 상관계수
	공분산을 이용하면 변수간 상관관계를 알 수 있지만 변수간 단위가 서로 다른 경우
	비교가 어렵다는 단점이 있다
	보완한 개념이 상관계수이다
	
	상관계수는 공분산을 각 변수의 표준 편차로 나눔으로써 구할 수 있다
	상관계수는 -1에서 1사이의 값을 가진다

5) 균일분포(uniform distribution)
	특정 범위 내에서 확률 분포가 균일한 분포를 의미
	이산형 균일 분포 : 모든 확률 변수의 확률값이 동일하다는 의미
	연속형 균일 분포 : 확률변수의 범위가 연속형임을 의미
	머신러닝에서 베이지안 방법론을 사용할 때 사전 정보가 없는 경우 초기 분포로 사용한다

6) 정규분포(normal distribution)
	가우시안분포, 종모양 분포라고도 한다
	정규 분포는 평균을 중심으로 대칭 형태를 띠는 종 모양 분포이다
	정규분포 확률 밀도 함수의 우변에서 x-μ / σ²부분은 머신러닝의 데이터 표준화와 일치함
	표준화를 진행한 데이터는 평균이 0이고 분산이 1인 분포를 따르게 된다

	표준 정규 분포는 평균이 0, 분산이 1인 것을 의미한다
	
---------------------------------------------------------------
3. 딥러닝
1) 신경망(Neural Network)
	머신러닝 알고리즘 중 하나
	인간의 뇌 기능을 흉내내려고 만들어짐
	입력층, 은닉층, 출력층으로 나뉘어져 있음
	층을 점점 늘려서 깊게 만든 신경망을 심층 신경망(Deep Nerual Network)라고한다
	깊은 층을 가진 신경망의 가중치를 학습시키는 것을 딥러닝 또는 심층학습이라고한다

	AI > 머신러닝(데이터 기반 : SVM, KNN, ...) > 딥러닝(신경망 : CNN, RNN, ..)

2) 신경망으로 할 수 있는 것
	분류
	회귀
	클러스터링
	이미지생성(영상)
	자연어처리
	...

3) 용어 정리
GPU : Graphics Processing Unit의 약자, 그래픽 처리 장치를 의미
	고성능 병렬처리를 위한 범용 연산장치

셀프 수퍼바이즈러닝 : 기계학습의 한 방법
	레이블이 없는 대규모의 데이터 셋을 사용하여 모델을 학습시키는 방법

BERT(Bidirectional Encoder Reprsentations from Transformers)
	구글에서 개발한 자연어 처리 모델
	딥러닝의 자연어 처리 분야에서 매우 중요한 모델 중 하나
	이전의 모델은 텍스트를 단방향 방식을 이용해서 처리,
	BERT는 양방향 방식을 도입한 첫번째 모델

인코더(Encoder)
	자연어 처리에서 주로 사용되는 구성 요소 중 하나
	주어진 입력문장을 더 의미있는 벡터 표현으로 변환하여 디코더로 전달하고
	다음 단어를 예측하거나 번역 등에 활용됨

퍼셉트론(Perceptron)
	인공 신경망 모형의 하나
	프랑크 로젠블라트가 1957년 제안한 초기 형태의 인공 신경망
	다수의 입력으로부터 하나의 결과를 내보내는 알고리즘
	다수의 신호(input)를 받아서 하나의 신호(output)를 출력함
	뉴런의 수상돌기나 축색돌기처럼 신호를 전달하는 역할을 퍼셉트론 가중치(weight)가 함
	가중치 : 각각의 입력 신호에 부여되어 입력신호와의 계산을 한 신호의 총 합
		정해진 임계값을 넘었을 때 1을 출력
	
출력 = 활성화 함수((입력 * 가중치) + (입력 * 가중치) +...+(입력n * 가중치n) + 편향)

4) 딥러닝 키포인트
	데이터 	: 양질, 많은 양
	모델	: CNN, RNN, Transformer, ..
	알고리즘 	: Gradient Descent를 기초로 많은 알고리즘 만들어짐
	Loss Function, Cost Function : 모델의 성능이 얼마나 좋은지에 대한 척도





 












