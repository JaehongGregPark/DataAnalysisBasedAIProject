day01
프로젝트 주제 선정 -> 연관규칙학습 -> 차원축소(PCA)

1) https://colab.research.google.com/drive/1i_08PCVwBTGmxE-h08eckfLNu7xlF8Rr?usp=sharing
2) https://colab.research.google.com/drive/1PDmKJ-V129Ge8jFHqsDNi9W9ob_cyAni?usp=sharing
3) https://colab.research.google.com/drive/1UwneLebXOfsE2ZafgjPTGAwhkujG1MNW?usp=sharing
4) https://colab.research.google.com/drive/1B-zbEM8SLfavtXcM5TY8IBx_kieoiSNg?usp=sharing

캐글(Kaggle)
https://www.kaggle.com/datasets

1교시)
project 폴더에 찾은 데이터셋 다운로드 받아서 파일넣기
(데이터셋 다운로드 받기 위해서는 캐글 가입 필요함)
txt 파일 하나 만들고 데이터셋 url과 어떤 것을 하고 싶은지 작성하기

메모장과 데이터셋 파일은 zip파일로 압축 후 -> 프로필 누르오 개인 카톡으로 전달

계획서 작성 => 주제, 일정, 개요(알고싶은 부분)
	ex) 품종분류 -> 클러스터링(특성) => 알고리즘

버섯.csv	=> 비지도학습, 지도학습

---------------------------------------------------------
1. 연관규칙학습
1) 사용예시
	상품 추천, 소비자 행동 분석, 신용카드 사기 탐지, 웹 사용자 행동 분석

2) 주요 단계
	데이터 수집
		거래데이터, 로그데이터 수집
		항목들의 집합으로 구성

	데이터 전처리
		중복된 항목 제거
		불필요한 정보 정리
		데이터를 트랜잭션 형태로 정리

	항목 집합 생성
		각 트랜잭션에서 어떤 항목들이 함께 발생하는지
		항목 집합을 통해 연관성 파악

	연관규칙 및 찾기
		Apriori 알고리즘 등을 활용해 연관규칙을 찾음
		빈도수 기반으로 연관성을 계산하고 지지도, 신뢰도, 향상도 등의 지표를 사용해서 규칙을 평가

	규칙 평가 및 필터링
		찾은 연관규칙 평가(지지도, 향상도, 신뢰도 등을 기준으로 규칙 선택)
		불필요한 규칙 제거, 중요한 규칙을 강조하기 위한 후처리 작업 수행

	결과 해석 및 활용
		찾은 연관 규칙을 해석
	
	주기적 업데이트
		연관성을 최신상태로 유지하기 위함
		데이터가 변경될 때 주기적으로 업데이트

2. 차원축소
	- PCA, t-SNE

1) PCA(Principal Component Analysis)
	데이터의 주요 정보를 보존하면서 데이터의 차원을 줄이는 것을 목표로 함
	데이터의 주성분(PC)을 찾아낸 뒤 데이터를 저차원 공간으로 투영함

동작방식
	데이터의 공분산 행렬 계산한다
	공분산 행렬의 고유값과 고유 벡터를 구한다
	고유값과 고유벡터를 이용하여 데이터의 주성분을 찾는다
	주성분 중에서 가장 중요한 주성분부터 선택하여 데이터를 새로운 공간으로 투영한다
	결과적으로 데이터의 차원이 줄어들고 주요 정보를 보존하는 특징을 갖는 저차원 표현을 얻게된다

장점
	빠르고 선형 변환 방식을 사용하므로 계산이 효율적이다
	데이터의 중요정보를 잘 보존하며, 데이터 구조를 유지한다

단점
	비선형 데이터에는 적용하기 어렵다
	데이터 분포를 고려하지 않는다

2) t-SNE(t-Distributed Stochastic Neihbor Embedding)
	데이터의 시각적 분포를 보존하고 클러스터링을 개선하는 것을 목표로 함
	데이터의 고차원 구조를 저차원 공간으로 매핑하여 시각화하는데 주로 사용됨

동작방식
	고차원 데이터간의 유사성을 측정하고, 저차원 공간에서의 유사성을 측정한다
	t-분포를 사용하여 확률분포로 모델링한다
	고차원과 저차원 간의 유사성을 비교하여 차원 축소를 수행한다
	결과적으로 데이터의 차원이 줄어들고, 데이터의 시각적 분포 및 클러스터링 특성을 보존한다

장점
	비선형 구조를 잘 표현하고, 시각적 분포를 보존할 수 있다
	시각화에 사용하기 좋고 데이터간의 관계를 시각적으로 이해할 수 있다

단점
	계산비용이 높다
	무작위성을 포함하므로 다양한 실행에서 다른 결과를 얻을 수 있다

3) 정규화(Normalization)
	데이터를 특정 범위로 변환하는 과정을 의미한다
	데이터의 스케일을 조정하여 모델이 더 잘 학습하고 예측할 수 있도록 도와주낟

	데이터를 특정 범위로 조정하여 값을 비교하기 쉽게 만든다
	
- 표준화(Standardization)
	각 특성의 평균을 빼고 표준편차로 나누어 특성의 값들을 평균이 0이 되고 표준편차 1인
	표준 정규 분포로 만드는 방법
	
	x = x-mean(x) / std(x)
	x 개별 데이터 포인트의 값, mean(x) 해당특성의 평균, std(x) 해당 특성의 표준편차

- 최소-최대 스케일링(Min-Max Scaling)
	각 특성의 최소값과 최대값을 사용하여 특성의 값을 원하는 범위로 조정하는 방법
	보통 [0, 1] [-1, 1] 범위로 조정
	
	x = x-min(x) / max(x) - min(x) x (max range) + (min range)

	x 개별 데이터 포인트의 값, min(x) 해당 특성의 최소값, max(x) 해당 특성의 최대값
	max range, min range 스케일링할 범위를 의미


- StandardScaler()
	사이킷런 라이브러리에서 제공하는 표준화를 수행하는 클래스
	데이터를 특성별로 표준화하여 각 특성의 평균이 0이되고 표준편차가 1이 되도록 변환한다

	fit(df)		: 데이터를 기반으로 평균과 표준편차를 계산하여 스케일링을 수행하기 위한 학습
	transform()	: 학습된 평균과 표준편차를 사용하여 데이터를 표준화된 형태로 변환
	fit_transform() : 주어진 데이터를 기반으로 평균과 표준편차를 계산하고
			데이터를 표준화된 형태로 변환한다

3. PCA 매개변수
n_components : 정수, 실수, mle(기본값 = None)
		주성분의 개수 지정
		정수로 지정시 해당 개수의 주성을 유지, 실수로 지정하면 주성분의 누적설명분산의 비율을 의미
	ex) 0.95 전체 데이터 분산 95%이상을 설명
	mle(Minimun Latent Information)방법을 사용하여 주성분 개수를 자동으로 선택함
copy	: bool 타입(기본값 = True)
	입력 데이터를 복사하여 변형함
	False로 설정시 입력데이터를 직접 변형한다
whiten : bool 타입(기본값 = False)
	True로 설정시 주성분데이터를 화이트닝
	주성분 데이터의 각 차원을 독립적으로 스케일링하여 평균이 0이 되도록 만듦
svd_soler : 문자열 타입(기본값 = 'auto')
	SVD(Singular Value Decomposition) 분해 계산 방법을 선택한다
	'auto', 'full', 'arpack', 'randomized'
tol	: 부동소수점 타입(기본값 = 0.0)
	주성분 개수를 자동으로 선택할 때 사용되는 공차
	mle모드에서 사용됨
iterated_power : 정수, auto(기본값 = 'auto')
	SDV 계산시 반복횟수를 지정함
random_state : 난수 발생 시드값

+) PCA 설명력
	각 주성분이 전체 데이터의 분산을 얼마나 설명하는지를 나타내는 비율
	설명력 : 각 주성분이 설명하는 분산의 비율
	전체 설명력 : 모든 주성분이 설명하는 분산의 총합

	ex) 설명력 0.95 => 해당 주성분들이 전체 데이터의 95%의 분산을 설명하는 것을 의미
	



































